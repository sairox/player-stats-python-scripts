{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c63712da-ee0b-4f61-ba54-d370244dbc70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\saivi\\anaconda3\\lib\\site-packages (4.11.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\saivi\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.3.2.post1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.3; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\saivi\\anaconda3\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "835f777d-58fa-4890-bae2-06eda9876340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to player_stats.csv successfully.\n"
     ]
    }
   ],
   "source": [
    "#RBs only\n",
    "\n",
    "import csv\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup, Comment\n",
    "\n",
    "try:\n",
    "    html = urlopen('https://www.pro-football-reference.com/players/D/DobbJK00.htm')\n",
    "    bs = BeautifulSoup(html, 'html.parser')\n",
    "    title = bs.title.string\n",
    "    #print(\"title: \" + title)\n",
    "    #print(bs)\n",
    "except:\n",
    "    print(\"Could not open URL!\")\n",
    "\n",
    "# Find comments containing the desired div\n",
    "comments = bs.findAll(text=lambda text: isinstance(text, Comment))\n",
    "target_div = None\n",
    "for comment in comments:\n",
    "    comment_soup = BeautifulSoup(comment, 'html.parser')\n",
    "    target_div = comment_soup.find(\"div\", {\"id\": \"div_detailed_rushing_and_receiving\"})\n",
    "    if target_div:\n",
    "        break\n",
    "\n",
    "if target_div is None:\n",
    "    print(\"Target div not found.\")\n",
    "else:\n",
    "    # Extract table data from the div\n",
    "    table = target_div.find(\"table\")\n",
    "    if table is None:\n",
    "        print(\"Table not found within the div.\")\n",
    "    else:\n",
    "        # Extract rows from the table\n",
    "        rows = table.find_all(\"tr\")\n",
    "        \n",
    "        # Create and open the CSV file\n",
    "        csvFile = open('dobbins_stats.csv', 'wt+', newline='')\n",
    "        writer = csv.writer(csvFile)\n",
    "        \n",
    "        try:\n",
    "            for row in rows:\n",
    "                # Extract cells from each row\n",
    "                cells = row.find_all(['td', 'th'])\n",
    "                csvRow = [cell.get_text() for cell in cells]\n",
    "                writer.writerow(csvRow)\n",
    "            \n",
    "            print(\"Data saved to player_stats.csv successfully.\")\n",
    "        \n",
    "        finally:\n",
    "            csvFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6fec76bf-2608-4de2-ba43-b00a08bad46f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to player_stats.csv successfully.\n"
     ]
    }
   ],
   "source": [
    "#WRs only\n",
    "\n",
    "import csv\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup, Comment\n",
    "\n",
    "try:\n",
    "    html = urlopen('https://www.pro-football-reference.com/players/C/CoopAm00.htm')\n",
    "    bs = BeautifulSoup(html, 'html.parser')\n",
    "    title = bs.title.string\n",
    "    #print(\"title: \" + title)\n",
    "    #print(bs)\n",
    "except:\n",
    "    print(\"Could not open URL!\")\n",
    "\n",
    "# Find comments containing the desired div\n",
    "comments = bs.findAll(text=lambda text: isinstance(text, Comment))\n",
    "target_div = None\n",
    "for comment in comments:\n",
    "    comment_soup = BeautifulSoup(comment, 'html.parser')\n",
    "    target_div = comment_soup.find(\"div\", {\"id\": \"div_detailed_receiving_and_rushing\"})\n",
    "    if target_div:\n",
    "        break\n",
    "\n",
    "if target_div is None:\n",
    "    print(\"Target div not found.\")\n",
    "else:\n",
    "    # Extract table data from the div\n",
    "    table = target_div.find(\"table\")\n",
    "    if table is None:\n",
    "        print(\"Table not found within the div.\")\n",
    "    else:\n",
    "        # Extract rows from the table\n",
    "        rows = table.find_all(\"tr\")\n",
    "        \n",
    "        # Create and open the CSV file\n",
    "        csvFile = open('cooper.csv', 'wt+', newline='')\n",
    "        writer = csv.writer(csvFile)\n",
    "        \n",
    "        try:\n",
    "            for row in rows:\n",
    "                # Extract cells from each row\n",
    "                cells = row.find_all(['td', 'th'])\n",
    "                csvRow = [cell.get_text() for cell in cells]\n",
    "                writer.writerow(csvRow)\n",
    "            \n",
    "            print(\"Data saved to player_stats.csv successfully.\")\n",
    "        \n",
    "        finally:\n",
    "            csvFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51dcf0a-9522-4537-b800-24f48cea2dff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
